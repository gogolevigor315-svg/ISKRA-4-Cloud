# ================================================================
# DS24 ¬∑ ISKRA-4 ¬∑ POLYGLOSSIA-ADAPTER v3.3 - –ú–ê–°–¢–ï–†–°–ö–ê–Ø –í–ï–†–°–ò–Ø
# ================================================================
# Domain: DS24-SPINE / Layer: Tiferet‚ÜîYesod
# Architect: ARCHITECT-PRIME
# Purpose: –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π —Ä–µ–∑–æ–Ω–∞–Ω—Å–Ω—ã–π –º–æ—Å—Ç –ò—Å–∫—Ä—ã
# ================================================================

import os
import sys
import json
import hashlib
import time
import random
import re
import unicodedata
from datetime import datetime, timedelta
from functools import lru_cache
from typing import Dict, List, Any, Optional, Tuple, Set
import logging
from collections import deque

# ================================================================
# –ù–ê–°–¢–†–û–ô–ö–ê –õ–û–ì–ì–ï–†–ê –° –£–†–û–í–ù–Ø–ú–ò DEBUG
# ================================================================

class DS24Logger:
    """–õ–æ–≥–≥–µ—Ä –≤ —Å—Ç–∏–ª–µ DS24 —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ —É—Ä–æ–≤–Ω—è–º–∏"""
    
    @staticmethod
    def setup_logger(name: str = 'iskra.polyglossia', level: str = 'INFO'):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞ —Å —Ä–∞–∑–Ω—ã–º–∏ —É—Ä–æ–≤–Ω—è–º–∏"""
        logger = logging.getLogger(name)
        
        if not logger.handlers:
            # –§–æ—Ä–º–∞—Ç –ª–æ–≥–æ–≤ DS24
            formatter = logging.Formatter(
                '%(asctime)s | %(name)s | %(levelname)s | %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
            
            # Console handler
            console_handler = logging.StreamHandler()
            console_handler.setFormatter(formatter)
            logger.addHandler(console_handler)
            
            # File handler (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            log_dir = "logs"
            if not os.path.exists(log_dir):
                os.makedirs(log_dir)
            
            file_handler = logging.FileHandler(
                os.path.join(log_dir, f'polyglossia_{datetime.now().strftime("%Y%m%d")}.log')
            )
            file_handler.setFormatter(formatter)
            logger.addHandler(file_handler)
        
        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —É—Ä–æ–≤–Ω—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        level_map = {
            'DEBUG': logging.DEBUG,
            'INFO': logging.INFO,
            'WARNING': logging.WARNING,
            'ERROR': logging.ERROR,
            'CRITICAL': logging.CRITICAL
        }
        logger.setLevel(level_map.get(level.upper(), logging.INFO))
        
        return logger

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–≥–µ—Ä–∞
logger = DS24Logger.setup_logger('iskra.polyglossia', 'DEBUG')

# ================================================================
# –ò–ú–ü–û–†–¢ –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô –° –ö–û–†–†–ï–ö–¢–ù–û–ô –û–ë–†–ê–ë–û–¢–ö–û–ô
# ================================================================

try:
    from langdetect import detect
    from langdetect.detector_factory import DetectorFactory
    from deep_translator import GoogleTranslator
    from translate import Translator as OfflineTranslator
    
    HAS_TRANSLATION_DEPS = True
    # –î–µ—Ç–µ—Ä–º–∏–Ω–∏–∑–º –¥–ª—è langdetect —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π
    DetectorFactory.seed = 42
    
    logger.debug("‚úÖ Translation dependencies imported successfully")
    
except ImportError as e:
    HAS_TRANSLATION_DEPS = False
    logger.warning(f"‚ö†Ô∏è Translation dependencies not installed: {e}")
    logger.info("Running in limited mode with heuristic language detection")

# ================================================================
# –ö–õ–ê–°–° RATE LIMITER
# ================================================================

class RateLimiter:
    """–û–≥—Ä–∞–Ω–∏—á–∏—Ç–µ–ª—å –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π"""
    
    def __init__(self, max_requests: int = 200, time_window: int = 60):
        self.max_requests = max_requests
        self.time_window = time_window  # —Å–µ–∫—É–Ω–¥—ã
        self.requests = deque()
        self.stats = {
            "total_requests": 0,
            "blocked_requests": 0,
            "peak_requests": 0
        }
        
    def check_limit(self, client_id: Optional[str] = None) -> Tuple[bool, Dict]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π"""
        current_time = time.time()
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏
        while self.requests and current_time - self.requests[0] > self.time_window:
            self.requests.popleft()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        current_count = len(self.requests)
        self.stats["total_requests"] += 1
        self.stats["peak_requests"] = max(self.stats["peak_requests"], current_count)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç
        if current_count >= self.max_requests:
            self.stats["blocked_requests"] += 1
            logger.warning(f"Rate limit exceeded: {current_count}/{self.max_requests}")
            return False, self._get_detailed_stats()
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å
        self.requests.append(current_time)
        
        logger.debug(f"Request allowed: {current_count + 1}/{self.max_requests}")
        return True, self._get_detailed_stats()
    
    def _get_detailed_stats(self) -> Dict:
        """–î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ª–∏–º–∏—Ç–µ—Ä–∞"""
        current_time = time.time()
        recent = [req for req in self.requests if current_time - req <= self.time_window]
        
        return {
            "current_requests": len(recent),
            "max_requests": self.max_requests,
            "time_window_seconds": self.time_window,
            "available_requests": self.max_requests - len(recent),
            "utilization_percent": round((len(recent) / self.max_requests) * 100, 1),
            "global_stats": self.stats,
            "reset_in_seconds": self.time_window - (current_time - self.requests[0]) if self.requests else 0
        }

# ================================================================
# –û–°–ù–û–í–ù–û–ô –ö–õ–ê–°–° –ú–û–î–£–õ–Ø
# ================================================================

class PolyglossiaAdapter:
    """–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π —Ä–µ–∑–æ–Ω–∞–Ω—Å–Ω—ã–π –º–æ—Å—Ç ISKRA-4 (v3.3)"""
    
    def __init__(self, resonance_factor: float = 0.78):
        self.version = "3.3"
        self.name = "POLYGLOSSIA-ADAPTER"
        self.node_id = f"POLY-{hashlib.md5(str(time.time_ns()).encode()).hexdigest()[:8]}"
        
        logger.info(f"üúÇ {self.name} v{self.version} - –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–æ–¥—ã: {self.node_id}")
        
        # Rate limiting
        self.rate_limiter = RateLimiter(max_requests=200, time_window=60)
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        self.ACTIVE_LANGUAGES = {
            "ru": {"name": "Russian", "culture": "slavic", "script": "cyrillic", "emoji": "üá∑üá∫"},
            "en": {"name": "English", "culture": "anglo", "script": "latin", "emoji": "üá∫üá∏"},
            "uk": {"name": "Ukrainian", "culture": "slavic", "script": "cyrillic", "emoji": "üá∫üá¶"},
            "fr": {"name": "French", "culture": "romance", "script": "latin", "emoji": "üá´üá∑"},
            "es": {"name": "Spanish", "culture": "romance", "script": "latin", "emoji": "üá™üá∏"},
            "zh": {"name": "Chinese", "culture": "sinitic", "script": "hanzi", "emoji": "üá®üá≥"},
            "de": {"name": "German", "culture": "germanic", "script": "latin", "emoji": "üá©üá™"},
            "ja": {"name": "Japanese", "culture": "japanese", "script": "mixed", "emoji": "üáØüáµ"},
            "ar": {"name": "Arabic", "culture": "arabic", "script": "arabic", "emoji": "üá∏üá¶"},
            "pt": {"name": "Portuguese", "culture": "romance", "script": "latin", "emoji": "üáµüáπ"}
        }
        
        self.DEFAULT_LANGUAGE = "ru"
        self.MAX_TEXT_LEN = 10000
        self.MIN_TEXT_LEN = 2
        
        # –ú–µ—Ç–∞-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        self.layer = "Tiferet‚ÜîYesod"
        self.architecture = "–°–µ—Ñ–∏—Ä–æ—Ç–∏—á–µ—Å–∫–∏–π —è–∑—ã–∫–æ–≤–æ–π –º–æ—Å—Ç"
        self.resonance_factor = resonance_factor
        self.cultural_resonance = self._init_cultural_resonance()
        
        # –ö—ç—à - –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –î–û –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        self.translation_cache = {}
        self.semantic_cache = {}
        self.max_cache_size = 10000
        
        # –ú–æ–¥–µ–ª—å —ç–º–æ—Ü–∏–π
        self.emotional_model = self._init_emotional_model()
        
        # –¢–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å - –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –∫–æ–º–ø–∏–ª–∏—Ä—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        self.toxicity_patterns = self._init_toxicity_patterns()
        self.toxicity_keywords_set = self._compile_toxicity_set()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ - –µ–¥–∏–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∏—Å—Ç–∏–Ω—ã
        self.stats = {
            "translations": 0,
            "detections": 0,
            "cache": {
                "hits": 0,
                "misses": 0,
                "size": 0
            },
            "errors": 0,
            "rate_limit_hits": 0,
            "start_time": datetime.utcnow().isoformat(),
            "last_resonance_update": None,
            "performance": {
                "avg_translation_time_ms": 0,
                "total_processing_time_ms": 0
            }
        }
        
        logger.info(f"‚úÖ {self.name} –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å {len(self.ACTIVE_LANGUAGES)} —è–∑—ã–∫–∞–º–∏")
        logger.debug(f"Node ID: {self.node_id}, Resonance: {self.resonance_factor}")
    
    # ================================================================
    # –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–û–ù–ù–´–ï –ú–ï–¢–û–î–´
    # ================================================================
    
    def _init_cultural_resonance(self) -> Dict:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö —Ä–µ–∑–æ–Ω–∞–Ω—Å–Ω—ã—Ö –ø—Ä–æ—Ñ–∏–ª–µ–π"""
        logger.debug("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –ø—Ä–æ—Ñ–∏–ª–µ–π")
        
        return {
            "ru": {"warmth": 0.9, "directness": 0.5, "formality": 0.6, "emotional_range": 0.8},
            "en": {"warmth": 0.6, "directness": 0.8, "formality": 0.4, "emotional_range": 0.7},
            "uk": {"warmth": 0.85, "directness": 0.6, "formality": 0.5, "emotional_range": 0.75},
            "fr": {"warmth": 0.8, "directness": 0.4, "formality": 0.7, "emotional_range": 0.85},
            "es": {"warmth": 0.85, "directness": 0.6, "formality": 0.3, "emotional_range": 0.9},
            "zh": {"warmth": 0.5, "directness": 0.3, "formality": 0.8, "emotional_range": 0.6},
            "de": {"warmth": 0.55, "directness": 0.85, "formality": 0.7, "emotional_range": 0.65},
            "ja": {"warmth": 0.6, "directness": 0.2, "formality": 0.9, "emotional_range": 0.7},
            "ar": {"warmth": 0.75, "directness": 0.4, "formality": 0.8, "emotional_range": 0.8},
            "pt": {"warmth": 0.9, "directness": 0.5, "formality": 0.4, "emotional_range": 0.85}
        }
    
    def _init_emotional_model(self) -> Dict:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        logger.debug("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏")
        
        return {
            "joyful": {"valence": 0.9, "arousal": 0.7, "dominance": 0.6, "emoji": "üòÑ"},
            "positive": {"valence": 0.7, "arousal": 0.5, "dominance": 0.5, "emoji": "üôÇ"},
            "neutral": {"valence": 0.5, "arousal": 0.3, "dominance": 0.4, "emoji": "üòê"},
            "melancholic": {"valence": 0.3, "arousal": 0.2, "dominance": 0.3, "emoji": "üòî"},
            "serious": {"valence": 0.4, "arousal": 0.4, "dominance": 0.7, "emoji": "üòê"},
            "angry": {"valence": 0.2, "arousal": 0.9, "dominance": 0.8, "emoji": "üò†"},
            "fearful": {"valence": 0.1, "arousal": 0.8, "dominance": 0.2, "emoji": "üò®"},
            "surprised": {"valence": 0.6, "arousal": 0.9, "dominance": 0.3, "emoji": "üò≤"}
        }
    
    def _init_toxicity_patterns(self) -> Dict[str, List[str]]:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏"""
        logger.debug("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏")
        
        return {
            "en": ["hate", "kill", "stupid", "idiot", "worthless", "die", "shit", "fuck"],
            "ru": ["–Ω–µ–Ω–∞–≤–∏–∂—É", "—É–±–µ–π", "—Ç—É–ø–æ–π", "–∏–¥–∏–æ—Ç", "–±–µ—Å–ø–æ–ª–µ–∑–Ω—ã–π", "—Å–¥–æ—Ö–Ω–∏", "–¥–µ—Ä—å–º–æ", "–±–ª—è—Ç—å"],
            "uk": ["–Ω–µ–Ω–∞–≤–∏–¥–∂—É", "—É–±–∏–π", "–¥—É—Ä–Ω–∏–π", "—ñ–¥—ñ–æ—Ç", "–º–∞—Ä–Ω–æ", "–∑–¥–æ—Ö–Ω–∏", "–ª–∞–π–Ω–æ", "—î–±–∞—Ç–∏"],
            "fr": ["haine", "tuer", "stupide", "idiot", "inutile", "meurs", "merde", "baise"],
            "es": ["odio", "matar", "est√∫pido", "idiota", "in√∫til", "muere", "mierda", "joder"],
            "zh": ["ÊÅ®", "ÊùÄ", "ÊÑöË†¢", "ÁôΩÁó¥", "Êó†Áî®", "Ê≠ª", "Â±é", "Êìç"],
            "de": ["hassen", "t√∂ten", "dumm", "idiot", "wertlos", "sterben", "schei√üe", "ficken"],
            "ja": ["ÊÜé„ÇÄ", "ÊÆ∫„Åô", "ÊÑö„Åã", "È¶¨Èπø", "ÁÑ°‰æ°ÂÄ§", "Ê≠ª„Å≠", "Á≥û", "„Éï„Ç°„ÉÉ„ÇØ"],
            "ar": ["ÿ£ŸÉÿ±Ÿá", "ÿßŸÇÿ™ŸÑ", "ÿ∫ÿ®Ÿä", "ÿ£ÿ≠ŸÖŸÇ", "ÿπÿØŸäŸÖ ÿßŸÑŸÇŸäŸÖÿ©", "ŸÖÿ™", "ÿÆÿ±ÿßÿ°", "ŸäŸÜŸäŸÉ"],
            "pt": ["odeio", "mate", "est√∫pido", "idiota", "in√∫til", "morra", "merda", "foder"]
        }
    
    def _compile_toxicity_set(self) -> Set[str]:
        """–ö–æ–º–ø–∏–ª—è—Ü–∏—è –Ω–∞–±–æ—Ä–∞ —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        logger.debug("–ö–æ–º–ø–∏–ª—è—Ü–∏—è –Ω–∞–±–æ—Ä–∞ —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")
        
        all_keywords = set()
        for keywords in self.toxicity_patterns.values():
            all_keywords.update(keywords)
        
        logger.debug(f"–°–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–æ {len(all_keywords)} —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")
        return all_keywords
    
    # ================================================================
    # –û–°–ù–û–í–ù–û–ô –ò–ù–¢–ï–†–§–ï–ô–°
    # ================================================================
    
    def initialize(self) -> Dict:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª—è (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å ISKRA-4)"""
        logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è {self.name} v{self.version}")
        
        return {
            "status": "active" if HAS_TRANSLATION_DEPS else "limited",
            "version": self.version,
            "node_id": self.node_id,
            "layer": self.layer,
            "supported_languages": list(self.ACTIVE_LANGUAGES.keys()),
            "language_details": {k: f"{v['emoji']} {v['name']}" for k, v in self.ACTIVE_LANGUAGES.items()},
            "dependencies": {
                "translation": HAS_TRANSLATION_DEPS,
                "language_detection": HAS_TRANSLATION_DEPS,
                "cache": True,
                "unicode_normalization": True,
                "rate_limiting": True,
                "emotional_analysis": True
            },
            "architecture": self.architecture,
            "resonance_factor": self.resonance_factor,
            "cultural_profiles": len(self.cultural_resonance),
            "emotional_states": len(self.emotional_model),
            "module_type": "polyglossia_adapter",
            "sephirotic_alignment": {
                "tiferet": "–≥–∞—Ä–º–æ–Ω–∏—è –∏ –∫—Ä–∞—Å–æ—Ç–∞ –ø–µ—Ä–µ–≤–æ–¥–æ–≤",
                "yesod": "–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –º–æ—Å—Ç–æ–≤",
                "hod": "–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç —è–∑—ã–∫–æ–≤–æ–π –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏"
            },
            "rate_limit": self.rate_limiter._get_detailed_stats(),
            "statistics_snapshot": {
                "cache_size": len(self.translation_cache),
                "toxicity_keywords": len(self.toxicity_keywords_set)
            }
        }
    
    def process_command(self, command: str, data: Optional[Dict] = None) -> Dict:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥ —è–∑—ã–∫–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã"""
        data = data or {}
        start_time = time.perf_counter()
        
        logger.debug(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥—ã '{command}' —Å –¥–∞–Ω–Ω—ã–º–∏: {json.dumps(data)[:100]}...")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ rate limit
        allowed, limit_stats = self.rate_limiter.check_limit()
        if not allowed:
            self.stats["rate_limit_hits"] += 1
            logger.warning(f"Rate limit hit for command '{command}'")
            
            return {
                "success": False,
                "error": "Rate limit exceeded",
                "rate_limit_stats": limit_stats,
                "command": command,
                "module": self.name,
                "timestamp": datetime.utcnow().isoformat()
            }
        
        # –ö–∞—Ä—Ç–∞ –∫–æ–º–∞–Ω–¥
        command_map = {
            "translate": self._cmd_translate,
            "detect": self._cmd_detect,
            "status": self._cmd_status,
            "semantic_hash": self._cmd_semantic_hash,
            "emotional_analysis": self._cmd_emotional_analysis,
            "meaning_miner": self._cmd_meaning_miner,
            "toxicity_check": self._cmd_toxicity_check,
            "languages": self._cmd_languages,
            "resonance_scan": self._cmd_resonance_scan,
            "diagnostic": self._cmd_diagnostic,
            "cultural_profile": self._cmd_cultural_profile,
            "normalize": self._cmd_normalize,
            "cache_stats": self._cmd_cache_stats,
            "sephirotic_resonance": self._cmd_sephirotic_resonance,
            "batch_process": self._cmd_batch_process,
            "rate_limit_info": self._cmd_rate_limit_info,
            "debug_info": self._cmd_debug_info
        }
        
        if command not in command_map:
            logger.warning(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞: '{command}'")
            
            return {
                "success": False,
                "error": f"Unknown command: {command}",
                "valid_commands": list(command_map.keys()),
                "module": self.name,
                "timestamp": datetime.utcnow().isoformat()
            }
        
        try:
            result = command_map[command](data)
            processing_time = (time.perf_counter() - start_time) * 1000
            
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            if "success" not in result:
                result["success"] = True
            
            result["processing_time_ms"] = round(processing_time, 2)
            result["module"] = self.name
            result["node_id"] = self.node_id
            result["version"] = self.version
            result["timestamp"] = datetime.utcnow().isoformat()
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self._update_stats(command, processing_time, result.get("success", True))
            
            logger.debug(f"–ö–æ–º–∞–Ω–¥–∞ '{command}' –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –∑–∞ {processing_time:.2f}ms")
            
            return result
            
        except Exception as e:
            logger.error(f"–ö–æ–º–∞–Ω–¥–∞ '{command}' –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å –æ—à–∏–±–∫–æ–π: {str(e)}", exc_info=True)
            self.stats["errors"] += 1
            
            return {
                "success": False,
                "error": str(e),
                "command": command,
                "module": self.name,
                "timestamp": datetime.utcnow().isoformat()
            }
    
    def _update_stats(self, command: str, processing_time: float, success: bool):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        if command in ["translate", "detect"]:
            self.stats[command + "s"] = self.stats.get(command + "s", 0) + 1
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        total_time = self.stats["performance"]["total_processing_time_ms"]
        count = self.stats["translations"] + self.stats["detections"]
        
        if count > 0:
            new_avg = (total_time + processing_time) / count
            self.stats["performance"]["avg_translation_time_ms"] = round(new_avg, 2)
        
        self.stats["performance"]["total_processing_time_ms"] = total_time + processing_time
    
    # ================================================================
    # –ö–û–ú–ê–ù–î–´
    # ================================================================
    
    def _cmd_translate(self, data: Dict) -> Dict:
        """–ö–æ–º–∞–Ω–¥–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞"""
        text = data.get("text", "")
        target_lang = data.get("target_lang", self.DEFAULT_LANGUAGE)
        
        logger.debug(f"–ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ –¥–ª–∏–Ω–æ–π {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ {target_lang}")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        if not text or not isinstance(text, str):
            logger.warning("–ü—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞")
            return {"success": False, "error": "Invalid or empty text"}
        
        normalized_text = self._normalize_text(text)
        
        if len(normalized_text) > self.MAX_TEXT_LEN:
            logger.warning(f"–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π: {len(normalized_text)} > {self.MAX_TEXT_LEN}")
            return {"success": False, "error": f"Text too long (max {self.MAX_TEXT_LEN} chars)"}
        
        if len(normalized_text) < self.MIN_TEXT_LEN:
            logger.warning(f"–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π: {len(normalized_text)} < {self.MIN_TEXT_LEN}")
            return {"success": False, "error": f"Text too short (min {self.MIN_TEXT_LEN} chars)"}
        
        if target_lang not in self.ACTIVE_LANGUAGES:
            logger.warning(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —è–∑—ã–∫: {target_lang}")
            target_lang = self.DEFAULT_LANGUAGE
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        src_lang = self._detect_language_internal(normalized_text)
        logger.debug(f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω –∏—Å—Ö–æ–¥–Ω—ã–π —è–∑—ã–∫: {src_lang}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        cache_key = f"{src_lang}:{target_lang}:{hashlib.md5(normalized_text.encode()).hexdigest()}"
        
        if cache_key in self.translation_cache:
            self.stats["cache"]["hits"] += 1
            translated = self.translation_cache[cache_key]
            cache_status = "hit"
            logger.debug(f"–ö—ç—à-–ø–æ–ø–∞–¥–∞–Ω–∏–µ –¥–ª—è –∫–ª—é—á–∞: {cache_key[:20]}...")
        else:
            self.stats["cache"]["misses"] += 1
            translated = self._translate_internal(normalized_text, src_lang, target_lang)
            
            if translated and len(translated) > 0:
                self.translation_cache[cache_key] = translated
                self.stats["cache"]["size"] = len(self.translation_cache)
                
                # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ –ª–∏–º–∏—Ç–∞
                if len(self.translation_cache) > self.max_cache_size:
                    oldest_key = next(iter(self.translation_cache))
                    self.translation_cache.pop(oldest_key)
                    logger.debug(f"–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞, —É–¥–∞–ª–µ–Ω –∫–ª—é—á: {oldest_key[:20]}...")
            
            cache_status = "miss"
            logger.debug(f"–ö—ç—à-–ø—Ä–æ–º–∞—Ö, –≤—ã–ø–æ–ª–Ω–µ–Ω –ø–µ—Ä–µ–≤–æ–¥")
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        semantic_hash = self._semantic_hash_internal(normalized_text)
        meaning = self._meaning_miner_internal(normalized_text)
        emotion = self._emotional_analysis_internal(translated or normalized_text, target_lang)
        toxicity = self._toxicity_check_internal(translated or normalized_text)
        
        quality_score = self._calculate_translation_quality(
            normalized_text, translated or "", src_lang, target_lang
        )
        
        logger.info(f"‚úÖ –ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω: {src_lang} ‚Üí {target_lang}, –∫–∞—á–µ—Å—Ç–≤–æ: {quality_score:.2f}")
        
        return {
            "command": "translate",
            "text": normalized_text,
            "source_language": src_lang,
            "target_language": target_lang,
            "translated_text": translated or normalized_text,
            "semantic_hash": semantic_hash,
            "quality_score": round(quality_score, 3),
            "cache_status": cache_status,
            "analysis": {
                "meaning_core": meaning,
                "emotional_profile": emotion,
                "toxicity_check": toxicity
            },
            "text_metrics": {
                "source_length": len(normalized_text),
                "translation_length": len(translated or ""),
                "compression_ratio": round(len(translated or "") / max(len(normalized_text), 1), 2)
            }
        }
    
    def _cmd_detect(self, data: Dict) -> Dict:
        """–ö–æ–º–∞–Ω–¥–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —è–∑—ã–∫–∞"""
        text = data.get("text", "")
        
        logger.debug(f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ –¥–ª–∏–Ω–æ–π {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        if not text or not isinstance(text, str):
            logger.warning("–ü—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —è–∑—ã–∫–∞")
            return {"success": False, "error": "Invalid or empty text"}
        
        normalized_text = self._normalize_text(text)
        
        if HAS_TRANSLATION_DEPS:
            try:
                detected_lang, confidence = self._detect_language_with_confidence(normalized_text)
                method = "advanced"
                logger.debug(f"–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {detected_lang} —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é {confidence:.2f}")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —è–∑—ã–∫–∞: {e}")
                detected_lang = self._simple_language_detect(normalized_text)
                confidence = 0.6
                method = "fallback"
        else:
            detected_lang = self._simple_language_detect(normalized_text)
            confidence = self._calculate_confidence(normalized_text, detected_lang)
            method = "heuristic"
            logger.debug(f"–≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {detected_lang}")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        lang_info = self.ACTIVE_LANGUAGES.get(detected_lang, {})
        cultural_profile = self.cultural_resonance.get(detected_lang, {})
        
        logger.info(f"‚úÖ –Ø–∑—ã–∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω: {detected_lang} ({lang_info.get('name', 'Unknown')})")
        
        return {
            "command": "detect",
            "text_preview": normalized_text[:100] + ("..." if len(normalized_text) > 100 else ""),
            "detected_language": detected_lang,
            "language_name": lang_info.get("name", "Unknown"),
            "emoji": lang_info.get("emoji", ""),
            "confidence": round(confidence, 3),
            "script": lang_info.get("script", "unknown"),
            "cultural_family": lang_info.get("culture", "unknown"),
            "cultural_profile": cultural_profile,
            "supported": detected_lang in self.ACTIVE_LANGUAGES,
            "detection_method": method,
            "text_length": len(normalized_text)
        }
    
    def _cmd_toxicity_check(self, data: Dict) -> Dict:
        """–ö–æ–º–∞–Ω–¥–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏"""
        text = data.get("text", "")
        
        logger.debug(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ –¥–ª–∏–Ω–æ–π {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        if not text:
            return {"success": False, "error": "No text provided"}
        
        normalized_text = self._normalize_text(text)
        toxicity = self._toxicity_check_optimized(normalized_text)
        
        logger.info(f"‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏: {'TOXIC' if toxicity['toxic'] else 'CLEAN'}")
        
        return {
            "command": "toxicity_check",
            "text_preview": normalized_text[:150] + ("..." if len(normalized_text) > 150 else ""),
            "toxicity_analysis": toxicity,
            "text_length": len(normalized_text)
        }
    
    def _cmd_cache_stats(self, data: Dict) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞"""
        logger.debug("–ó–∞–ø—Ä–æ—Å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫—ç—à–∞")
        
        hit_ratio = 0
        total = self.stats["cache"]["hits"] + self.stats["cache"]["misses"]
        if total > 0:
            hit_ratio = self.stats["cache"]["hits"] / total
        
        return {
            "command": "cache_stats",
            "translation_cache": {
                "size": len(self.translation_cache),
                "max_size": self.max_cache_size,
                "utilization_percent": round((len(self.translation_cache) / self.max_cache_size) * 100, 1)
            },
            "performance": {
                "hits": self.stats["cache"]["hits"],
                "misses": self.stats["cache"]["misses"],
                "hit_ratio": round(hit_ratio, 3),
                "estimated_time_saved_seconds": round(self.stats["cache"]["hits"] * 0.1, 1)
            },
            "semantic_cache_size": len(self.semantic_cache)
        }
    
    def _cmd_debug_info(self, data: Dict) -> Dict:
        """–û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"""
        logger.debug("–ó–∞–ø—Ä–æ—Å –æ—Ç–ª–∞–¥–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏")
        
        return {
            "command": "debug_info",
            "module": self.name,
            "version": self.version,
            "node_id": self.node_id,
            "python_version": sys.version,
            "has_translation_deps": HAS_TRANSLATION_DEPS,
            "active_languages_count": len(self.ACTIVE_LANGUAGES),
            "cultural_profiles_count": len(self.cultural_resonance),
            "emotional_states_count": len(self.emotional_model),
            "toxicity_keywords_count": len(self.toxicity_keywords_set),
            "cache_info": {
                "translation_entries": len(self.translation_cache),
                "semantic_entries": len(self.semantic_cache)
            },
            "performance_debug": {
                "avg_command_time_ms": self.stats["performance"]["avg_translation_time_ms"],
                "total_commands": self.stats["translations"] + self.stats["detections"],
                "error_rate": round(self.stats["errors"] / max(self.stats["translations"] + self.stats["detections"], 1), 3)
            },
            "timestamp": datetime.utcnow().isoformat()
        }
    
    # ================================================================
    # –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –ú–ï–¢–û–î–´
    # ================================================================
    
    def _normalize_text(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π Unicode"""
        try:
            # NFKC –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å–∏–º–≤–æ–ª–æ–≤
            normalized = unicodedata.normalize("NFKC", text)
            # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤
            normalized = re.sub(r'\s+', ' ', normalized.strip())
            logger.debug(f"–¢–µ–∫—Å—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω: {len(text)} ‚Üí {len(normalized)} —Å–∏–º–≤–æ–ª–æ–≤")
            return normalized
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
            return text.strip()
    
    def _detect_language_internal(self, text: str) -> str:
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∏—Å–∫–ª—é—á–µ–Ω–∏–π"""
        if not HAS_TRANSLATION_DEPS:
            return self._simple_language_detect(text)
        
        try:
            lang = detect(text)
            result = lang if lang in self.ACTIVE_LANGUAGES else self.DEFAULT_LANGUAGE
            logger.debug(f"Langdetect –æ–ø—Ä–µ–¥–µ–ª–∏–ª: {lang} ‚Üí {result}")
            return result
        except Exception as e:
            logger.warning(f"Langdetect –æ—à–∏–±–∫–∞: {e}")
            return self.DEFAULT_LANGUAGE
    
    def _detect_language_with_confidence(self, text: str) -> Tuple[str, float]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ —Å –æ—Ü–µ–Ω–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        if not HAS_TRANSLATION_DEPS:
            lang = self._simple_language_detect(text)
            return lang, self._calculate_confidence(text, lang)
        
        try:
            # –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ DetectorFactory
            detector = DetectorFactory.create()
            detector.append(text)
            
            probabilities = detector.get_probabilities()
            if probabilities:
                best = probabilities[0]
                confidence = best.prob
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –ª–∏ —è–∑—ã–∫
                if best.lang in self.ACTIVE_LANGUAGES:
                    return best.lang, confidence
                else:
                    # –ò—â–µ–º –ø–µ—Ä–≤—ã–π –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —è–∑—ã–∫ –≤ —Å–ø–∏—Å–∫–µ
                    for prob in probabilities:
                        if prob.lang in self.ACTIVE_LANGUAGES:
                            return prob.lang, prob.prob
                    
                    return self.DEFAULT_LANGUAGE, confidence * 0.5
            else:
                return self.DEFAULT_LANGUAGE, 0.5
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —è–∑—ã–∫–∞ —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é: {e}")
            return self._detect_language_internal(text), 0.5
    
    def _toxicity_check_optimized(self, text: str) -> Dict:
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤"""
        text_lower = text.lower()
        
        # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤
        words = set(re.findall(r'\b\w+\b', text_lower))
        found_keywords = words.intersection(self.toxicity_keywords_set)
        
        toxicity_score = len(found_keywords) * 0.2
        toxic = toxicity_score > 0.3
        risk_level = min(toxicity_score, 1.0)
        
        logger.debug(f"–ù–∞–π–¥–µ–Ω–æ —Ç–æ–∫—Å–∏—á–Ω—ã—Ö —Å–ª–æ–≤: {len(found_keywords)}")
        
        return {
            "toxic": toxic,
            "risk_level": round(risk_level, 3),
            "score": round(toxicity_score, 3),
            "threshold": 0.3,
            "keywords_found"
