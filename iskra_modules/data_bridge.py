#!/usr/bin/env python3
# ================================================================
# DATA-BRIDGE 3.2-sephirotic-reflective ¬∑ PERFECTED EDITION
# ================================================================
# Module: DATA-BRIDGE ¬∑ Domain: ISKRA3-SPINE
# Layer: SCA ¬∑ Sephirotic Input Spine ¬∑ DS24-Centric
# ================================================================
# Lineage: DS24 ¬∑ Heritage: SEPHIROTIC-SPEC ¬∑ Generation: G3 ¬∑ ISKRA 3
# Brand: GOGOL SYSTEMS ¬∑ Source: DS24-SPINE
# Architect: ARCHITECT-PRIME ¬∑ Authority: absolute
# ================================================================

import os
import sys
import json
import asyncio
import hashlib
import time
import uuid
import logging
import threading
import shutil
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Callable
from dataclasses import dataclass, asdict
from enum import Enum
from concurrent.futures import ThreadPoolExecutor
from functools import wraps
import inspect

# ================================================================
# ADVANCED LOGGING SYSTEM
# ================================================================

class EmotionalLogger:
    """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –¥–ª—è ISKRA-4"""
    
    def __init__(self, module_name: str = "DATA-BRIDGE"):
        self.module_name = module_name
        self.logger = logging.getLogger(f"ISKRA-4.{module_name}")
        self.logger.setLevel(logging.INFO)
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤—â–∏–∫ —Å —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
        self.formatter = logging.Formatter(
            '[%(asctime)s] [%(levelname)s] [%(module)s.%(funcName)s] üåå %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        
        # –§–∞–π–ª–æ–≤—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
        os.makedirs("logs", exist_ok=True)
        file_handler = logging.FileHandler(f"logs/{module_name.lower()}.log", encoding='utf-8')
        file_handler.setFormatter(self.formatter)
        self.logger.addHandler(file_handler)
        
        # –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ (—Ç–æ–ª—å–∫–æ –¥–ª—è debug)
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(self.formatter)
        console_handler.setLevel(logging.WARNING)
        self.logger.addHandler(console_handler)
        
        # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —É—Ä–æ–≤–Ω–∏
        self.emotional_levels = {
            'INFO': 'üåÄ',
            'WARNING': '‚ö†Ô∏è',
            'ERROR': 'üí•',
            'CRITICAL': 'üî•',
            'DEBUG': 'üîç',
            'HEARTBEAT': '‚ù§Ô∏è',
            'RESONANCE': '‚ú®'
        }
        
        self.logger.info(f"{self.emotional_levels['INFO']} {module_name} Emotional Logger –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def log_with_emotion(self, level: str, message: str, emotion: str = None, **kwargs):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        emotion_marker = self.emotional_levels.get(level, 'üìù')
        if emotion:
            emotion_marker = f"{emotion_marker} [{emotion}]"
        
        full_message = f"{emotion_marker} {message}"
        if kwargs:
            full_message += f" | {json.dumps(kwargs, ensure_ascii=False)}"
        
        log_method = getattr(self.logger, level.lower(), self.logger.info)
        log_method(full_message)
    
    def info(self, message: str, emotion: str = None, **kwargs):
        self.log_with_emotion('INFO', message, emotion, **kwargs)
    
    def warning(self, message: str, emotion: str = None, **kwargs):
        self.log_with_emotion('WARNING', message, emotion, **kwargs)
    
    def error(self, message: str, emotion: str = None, **kwargs):
        self.log_with_emotion('ERROR', message, emotion, **kwargs)
    
    def heartbeat(self, message: str, **kwargs):
        self.log_with_emotion('HEARTBEAT', message, **kwargs)
    
    def resonance(self, message: str, **kwargs):
        self.log_with_emotion('RESONANCE', message, **kwargs)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –ª–æ–≥–≥–µ—Ä
logger = EmotionalLogger("DATA-BRIDGE-3.2")

# ================================================================
# VALIDATION SYSTEM
# ================================================================

@dataclass
class ValidationResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    valid: bool
    errors: List[str]
    warnings: List[str]
    detected_structure: Dict[str, Any]
    
    def to_dict(self) -> Dict:
        return {
            "valid": self.valid,
            "errors": self.errors,
            "warnings": self.warnings,
            "structure": self.detected_structure,
            "timestamp": datetime.utcnow().isoformat()
        }

class DS24InputValidator:
    """–í–∞–ª–∏–¥–∞—Ç–æ—Ä –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å —Å–µ—Ñ–∏—Ä–æ—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–º–∞–Ω—Ç–∏–∫–æ–π"""
    
    REQUIRED_FIELDS = [
        "id", "ts", "intent_id", "policy_ref", "trace_id",
        "span_id", "sig", "topic", "payload"
    ]
    
    FIELD_TYPES = {
        "id": str,
        "ts": (str, int, float),
        "intent_id": str,
        "policy_ref": str,
        "trace_id": str,
        "span_id": str,
        "sig": str,
        "topic": str,
        "payload": (dict, list, str, int, float, bool)
    }
    
    def __init__(self):
        self.validation_cache = {}
        self.cache_hits = 0
        self.cache_misses = 0
    
    def validate(self, data: Dict) -> ValidationResult:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        input_hash = hashlib.md5(json.dumps(data, sort_keys=True).encode()).hexdigest()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        if input_hash in self.validation_cache:
            self.cache_hits += 1
            return self.validation_cache[input_hash]
        
        self.cache_misses += 1
        errors = []
        warnings = []
        detected_structure = {}
        
        try:
            # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
            for field in self.REQUIRED_FIELDS:
                if field not in data:
                    errors.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ: {field}")
                else:
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞
                    expected_type = self.FIELD_TYPES.get(field)
                    if expected_type and not isinstance(data[field], expected_type):
                        errors.append(f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø –ø–æ–ª—è {field}: –æ–∂–∏–¥–∞–µ—Ç—Å—è {expected_type}, –ø–æ–ª—É—á–µ–Ω {type(data[field])}")
            
            # 2. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
            if not errors:
                detected_structure = self._analyze_structure(data)
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–≥–Ω–∞—Ç—É—Ä—ã
                if "sig" in data:
                    sig_valid = self._validate_signature(data)
                    if not sig_valid:
                        warnings.append("–°–∏–≥–Ω–∞—Ç—É—Ä–∞ –Ω–µ –ø—Ä–æ—à–ª–∞ –ø—Ä–æ–≤–µ—Ä–∫—É, –Ω–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è")
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–∏
                if "ts" in data:
                    ts_valid = self._validate_timestamp(data["ts"])
                    if not ts_valid:
                        warnings.append("–í—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞ –≤–Ω–µ –¥–æ–ø—É—Å—Ç–∏–º–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞")
            
            # 3. –ê–Ω–∞–ª–∏–∑ –Ω–∞–≥—Ä—É–∑–∫–∏
            if "payload" in data:
                payload_analysis = self._analyze_payload(data["payload"])
                detected_structure["payload_analysis"] = payload_analysis
                
                if payload_analysis.get("complexity") == "high":
                    warnings.append("–í—ã—Å–æ–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å payload, –≤–æ–∑–º–æ–∂–Ω—ã –∑–∞–¥–µ—Ä–∂–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            
            result = ValidationResult(
                valid=len(errors) == 0,
                errors=errors,
                warnings=warnings,
                detected_structure=detected_structure
            )
            
            # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            if len(errors) == 0:  # –ö—ç—à–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                self.validation_cache[input_hash] = result
                # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∫—ç—à–∞
                if len(self.validation_cache) > 1000:
                    oldest_key = next(iter(self.validation_cache))
                    del self.validation_cache[oldest_key]
            
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {str(e)}", emotion="confusion")
            return ValidationResult(
                valid=False,
                errors=[f"–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {str(e)}"],
                warnings=[],
                detected_structure={}
            )
    
    def _analyze_structure(self, data: Dict) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö"""
        return {
            "field_count": len(data),
            "nested_depth": self._calculate_nesting_depth(data),
            "data_size_bytes": len(json.dumps(data).encode()),
            "unique_field_pattern": hashlib.sha256(
                "".join(sorted(data.keys())).encode()
            ).hexdigest()[:8]
        }
    
    def _calculate_nesting_depth(self, obj, current_depth=0, max_depth=10):
        """–†–∞—Å—á—ë—Ç –≥–ª—É–±–∏–Ω—ã –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏"""
        if not isinstance(obj, dict) or current_depth >= max_depth:
            return current_depth
        
        max_child_depth = current_depth
        for value in obj.values():
            if isinstance(value, dict):
                child_depth = self._calculate_nesting_depth(value, current_depth + 1, max_depth)
                max_child_depth = max(max_child_depth, child_depth)
            elif isinstance(value, list):
                for item in value:
                    if isinstance(item, dict):
                        child_depth = self._calculate_nesting_depth(item, current_depth + 1, max_depth)
                        max_child_depth = max(max_child_depth, child_depth)
        
        return max_child_depth
    
    def _validate_signature(self, data: Dict) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–≥–Ω–∞—Ç—É—Ä—ã (—É–ø—Ä–æ—â—ë–Ω–Ω–∞—è)"""
        try:
            # –í —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –∑–¥–µ—Å—å –±—ã–ª–∞ –±—ã –∫—Ä–∏–ø—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
            sig = data.get("sig", "")
            return len(sig) >= 8 and sig.startswith("DS24_")
        except:
            return False
    
    def _validate_timestamp(self, timestamp) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–∏"""
        try:
            if isinstance(timestamp, (int, float)):
                ts_dt = datetime.fromtimestamp(timestamp)
            else:
                ts_dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
            
            now = datetime.utcnow()
            delta = abs((now - ts_dt).total_seconds())
            
            # –î–æ–ø—É—Å—Ç–∏–º–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: 5 –º–∏–Ω—É—Ç
            return delta <= 300
        except:
            return False
    
    def _analyze_payload(self, payload) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ payload"""
        if isinstance(payload, dict):
            size = len(json.dumps(payload).encode())
            complexity = "high" if size > 10000 else "medium" if size > 1000 else "low"
            
            return {
                "type": "object",
                "key_count": len(payload),
                "size_bytes": size,
                "complexity": complexity,
                "hash": hashlib.md5(json.dumps(payload, sort_keys=True).encode()).hexdigest()[:12]
            }
        elif isinstance(payload, list):
            return {
                "type": "array",
                "length": len(payload),
                "size_bytes": len(json.dumps(payload).encode()),
                "complexity": "medium",
                "element_types": list(set(type(x).__name__ for x in payload))
            }
        else:
            return {
                "type": type(payload).__name__,
                "size_bytes": len(str(payload).encode()),
                "complexity": "low"
            }
    
    def get_stats(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–∞"""
        return {
            "cache_hits": self.cache_hits,
            "cache_misses": self.cache_misses,
            "cache_size": len(self.validation_cache),
            "hit_rate": self.cache_hits / max(self.cache_hits + self.cache_misses, 1),
            "timestamp": datetime.utcnow().isoformat()
        }

# ================================================================
# ASYNCHRONOUS ROUTING ENGINE
# ================================================================

class AsyncSephiroticRouter:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä —Å —Å–µ—Ñ–∏—Ä–æ—Ç–∏—á–µ—Å–∫–∏–º –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ–º"""
    
    def __init__(self):
        self.mirror_rules = [
            (r"^mind", "binah", 0.8),
            (r"^intuition", "chokhmah", 0.9),
            (r"^moral", "gevurah", 0.7),
            (r"^arena", "netzach", 0.6),
            (r"^observe", "hod", 0.8)
        ]
        
        self.target_map = {
            "governance": ["CORE-GOVX"],
            "spirit": ["SPIRIT-CORE"],
            "risk": ["RADAR-ENGINE"],
            "analytic": ["ANALYTICS-MEGAFORGE", "ISKRA-MIND"],
            "intuition": ["INTUITION-MATRIX"],
            "emotion": ["EMOTION-OPTIMIZER"],
            "arena": ["ARENA-OPS"],
            "observability": ["OBSERVE+", "BLUEPRINT-RENDER"],
            "output": ["OUTPUT-LAYER"]
        }
        
        self.flow_patterns = {
            "simple": ["DATA-BRIDGE", "ISKRA-MIND", "LINEAR-ASSIST", "OUTPUT-LAYER"],
            "analytical": ["DATA-BRIDGE", "ISKRA-MIND", "ANALYTICS-MEGAFORGE", "LINEAR-ASSIST", "OUTPUT-LAYER"],
            "intuitive": ["DATA-BRIDGE", "ISKRA-MIND", "INTUITION-MATRIX", "LINEAR-ASSIST", "OUTPUT-LAYER"],
            "reflective": ["DATA-BRIDGE", "MIRROR-LOOP:2", "LINEAR-ASSIST", "OUTPUT-LAYER"],
            "infinite": ["DATA-BRIDGE", "MIRROR-LOOP:3", "COLLAPSE", "LINEAR-ASSIST", "OUTPUT-LAYER"]
        }
        
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.route_cache = {}
        logger.info("–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω", emotion="anticipation")
    
    async def route_async(self, data: Dict, flow_type: str = None) -> Dict:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        route_key = hashlib.md5(json.dumps(data, sort_keys=True).encode()).hexdigest()
        
        if route_key in self.route_cache:
            cached = self.route_cache[route_key]
            logger.resonance(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–∞—Ä—à—Ä—É—Ç: {route_key[:8]}")
            return cached
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á
        tasks = [
            self._detect_intent_type(data),
            self._activate_mirrors_async(data.get("topic", "")),
            self._analyze_payload_complexity(data.get("payload", {})),
            self._calculate_routing_score(data)
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        intent_type = results[0] if not isinstance(results[0], Exception) else "simple"
        mirrors = results[1] if not isinstance(results[1], Exception) else []
        complexity = results[2] if not isinstance(results[2], Exception) else "low"
        score = results[3] if not isinstance(results[3], Exception) else 0.5
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–Ω–µ—á–Ω–æ–≥–æ –º–∞—Ä—à—Ä—É—Ç–∞
        final_flow_type = flow_type or intent_type
        route_path = self.flow_patterns.get(final_flow_type, self.flow_patterns["simple"])
        
        # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –º–∞—Ä—à—Ä—É—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        if complexity == "high" and "ANALYTICS-MEGAFORGE" not in route_path:
            route_path.insert(2, "ANALYTICS-MEGAFORGE")
        
        routing_result = {
            "execution_id": str(uuid.uuid4()),
            "module": "async_router",
            "status": "success",
            "timestamp": datetime.utcnow().isoformat(),
            "payload": {
                "intent_type": intent_type,
                "flow_type": final_flow_type,
                "route_path": route_path,
                "mirrors_activated": mirrors,
                "complexity": complexity,
                "routing_score": score,
                "cache_key": route_key,
                "target_modules": self._resolve_target_modules(intent_type),
                "estimated_latency_ms": self._estimate_latency(complexity, len(mirrors))
            }
        }
        
        # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        self.route_cache[route_key] = routing_result
        if len(self.route_cache) > 500:
            # –£–¥–∞–ª—è–µ–º —Å–∞–º—ã–µ —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏
            keys_to_remove = list(self.route_cache.keys())[:100]
            for key in keys_to_remove:
                del self.route_cache[key]
        
        logger.info(f"–ú–∞—Ä—à—Ä—É—Ç –æ–ø—Ä–µ–¥–µ–ª—ë–Ω: {final_flow_type}", emotion="clarity")
        return routing_result
    
    async def _detect_intent_type(self, data: Dict) -> str:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏—è"""
        await asyncio.sleep(0.001)  # –ò–º–∏—Ç–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        
        topic = data.get("topic", "").lower()
        intent_id = data.get("intent_id", "").lower()
        
        if any(x in topic for x in ["analytic", "data", "pattern"]):
            return "analytical"
        elif any(x in topic for x in ["intuit", "hidden", "pattern"]):
            return "intuitive"
        elif any(x in topic for x in ["reflect", "mirror", "loop"]):
            return "reflective"
        elif "infinite" in topic:
            return "infinite"
        elif any(x in intent_id for x in ["emergency", "critical", "alert"]):
            return "emergency"
        else:
            return "simple"
    
    async def _activate_mirrors_async(self, topic: str) -> List[Dict]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è –∑–µ—Ä–∫–∞–ª"""
        await asyncio.sleep(0.0005)
        
        import re
        mirrors = []
        
        for pattern, sefira, confidence in self.mirror_rules:
            if re.match(pattern, topic, re.IGNORECASE):
                mirror = {
                    "sefira": sefira,
                    "pattern": pattern,
                    "topic_match": topic,
                    "confidence": confidence,
                    "activation_time": datetime.utcnow().isoformat(),
                    "status": "active"
                }
                mirrors.append(mirror)
                
                logger.resonance(f"–ó–µ—Ä–∫–∞–ª–æ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–æ: {sefira} –¥–ª—è —Ç–µ–º—ã '{topic}'")
        
        return mirrors
    
    async def _analyze_payload_complexity(self, payload) -> str:
        """–ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ payload"""
        await asyncio.sleep(0.0005)
        
        if isinstance(payload, dict):
            size = len(json.dumps(payload).encode())
            if size > 50000:
                return "very_high"
            elif size > 10000:
                return "high"
            elif size > 1000:
                return "medium"
            else:
                return "low"
        else:
            return "low"
    
    async def _calculate_routing_score(self, data: Dict) -> float:
        """–†–∞—Å—á—ë—Ç —Å–∫–æ—Ä–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏"""
        await asyncio.sleep(0.0005)
        
        score = 0.5
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å–∫–æ—Ä–∏–Ω–≥ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if isinstance(data.get("payload"), dict):
            score += 0.2
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –≤–∞–ª–∏–¥–Ω—ã—Ö —Å–∏–≥–Ω–∞—Ç—É—Ä
        if data.get("sig", "").startswith("DS24_"):
            score += 0.1
        
        # –£–º–µ–Ω—å—à–∞–µ–º –¥–ª—è —Å—Ç–∞—Ä—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫
        if "ts" in data:
            try:
                if isinstance(data["ts"], (int, float)):
                    ts_age = time.time() - data["ts"]
                    if ts_age > 3600:  # –°—Ç–∞—Ä–µ–µ —á–∞—Å–∞
                        score -= 0.1
            except:
                pass
        
        return max(0.1, min(1.0, score))
    
    def _resolve_target_modules(self, intent_type: str) -> List[str]:
        """–†–∞–∑—Ä–µ—à–µ–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –º–æ–¥—É–ª–µ–π"""
        targets = self.target_map.get(intent_type, [])
        if not targets:
            # –§–æ–ª–±—ç–∫ –Ω–∞ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –º–∞—Ä—à—Ä—É—Ç
            targets = self.target_map.get("analytic", ["ISKRA-MIND"])
        
        return targets
    
    def _estimate_latency(self, complexity: str, mirror_count: int) -> int:
        """–û—Ü–µ–Ω–∫–∞ –∑–∞–¥–µ—Ä–∂–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        base_latency = 10  # –º—Å
        complexity_multiplier = {
            "low": 1,
            "medium": 2,
            "high": 4,
            "very_high": 8
        }.get(complexity, 2)
        
        mirror_penalty = mirror_count * 5
        
        return base_latency * complexity_multiplier + mirror_penalty
    
    def get_router_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä–∞"""
        return {
            "cache_size": len(self.route_cache),
            "thread_pool_workers": self.executor._max_workers,
            "active_tasks": threading.active_count(),
            "timestamp": datetime.utcnow().isoformat()
        }

# ================================================================
# ENHANCED IDEMPOTENCY ENGINE
# ================================================================

class ResilientIdempotencyEngine:
    """–£—Å—Ç–æ–π—á–∏–≤—ã–π –¥–≤–∏–∂–æ–∫ –∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ —Å —Ä–µ–∑–µ—Ä–≤–Ω—ã–º –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    
    def __init__(self, store_path: str = "state/idempotent_index.jsonl", 
                 backup_path: str = "state/backups/"):
        self.store_path = store_path
        self.backup_path = backup_path
        self.dedup_window_sec = 7200
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
        os.makedirs(os.path.dirname(store_path), exist_ok=True)
        os.makedirs(backup_path, exist_ok=True)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
        self.index = self._load_or_recover_index()
        self.backup_schedule = time.time() + 300  # –ö–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç
        
        logger.info(f"ResilientIdempotencyEngine –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {len(self.index)} –∑–∞–ø–∏—Å–µ–π", 
                   emotion="stability")
    
    def _load_or_recover_index(self) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ —Å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ–º –ø—Ä–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–∏"""
        try:
            # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            if os.path.exists(self.store_path):
                index = {}
                with open(self.store_path, 'r', encoding='utf-8') as f:
                    for line_num, line in enumerate(f, 1):
                        line = line.strip()
                        if line:
                            try:
                                entry = json.loads(line)
                                index[entry["key"]] = entry
                            except json.JSONDecodeError as e:
                                logger.warning(f"–ü–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ {line_num}: {e}", emotion="concern")
                                continue
                
                logger.info(f"–ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω: {len(index)} –∑–∞–ø–∏—Å–µ–π")
                return index
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
            logger.info("–ò–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞—ë—Ç—Å—è –Ω–æ–≤—ã–π")
            return {}
            
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}", emotion="alarm")
            
            # –ü–æ–ø—ã—Ç–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏–∑ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏
            return self._recover_from_backup()
    
    def _recover_from_backup(self) -> Dict:
        """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–∑ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏"""
        backup_files = sorted([f for f in os.listdir(self.backup_path) 
                             if f.startswith("idempotent_backup_")])
        
        if backup_files:
            latest_backup = os.path.join(self.backup_path, backup_files[-1])
            try:
                with open(latest_backup, 'r', encoding='utf-8') as f:
                    index = json.load(f)
                logger.info(f"–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –∏–∑ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏: {latest_backup}", emotion="relief")
                return index
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏–∑ {latest_backup}: {e}", emotion="distress")
        
        # –ï—Å–ª–∏ —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏–π –Ω–µ—Ç –∏–ª–∏ –æ–Ω–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω—ã
        logger.warning("–†–µ–∑–µ—Ä–≤–Ω—ã–µ –∫–æ–ø–∏–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, —Å–æ–∑–¥–∞—ë—Ç—Å—è –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å", emotion="resignation")
        return {}
    
    def _create_backup(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏ –∏–Ω–¥–µ–∫—Å–∞"""
        try:
            backup_file = os.path.join(
                self.backup_path, 
                f"idempotent_backup_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.json"
            )
            
            # –°–æ–∑–¥–∞—ë–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é
            backup_data = {
                "timestamp": datetime.utcnow().isoformat(),
                "entry_count": len(self.index),
                "entries": list(self.index.values())
            }
            
            with open(backup_file, 'w', encoding='utf-8') as f:
                json.dump(backup_data, f, indent=2, ensure_ascii=False)
            
            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏–π
            backup_files = sorted([f for f in os.listdir(self.backup_path) 
                                 if f.startswith("idempotent_backup_")])
            if len(backup_files) > 10:
                for old_file in backup_files[:-10]:
                    os.remove(os.path.join(self.backup_path, old_file))
            
            logger.heartbeat(f"–°–æ–∑–¥–∞–Ω–∞ —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è: {backup_file}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏: {e}", emotion="frustration")
    
    def _clean_old_entries(self):
        """–û—á–∏—Å—Ç–∫–∞ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π"""
        now = time.time()
        keys_to_delete = []
        
        for key, entry in self.index.items():
            if now - entry["timestamp"] > self.dedup_window_sec:
                keys_to_delete.append(key)
        
        for key in keys_to_delete:
            del self.index[key]
        
        if keys_to_delete:
            logger.info(f"–û—á–∏—â–µ–Ω–æ {len(keys_to_delete)} —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π", emotion="cleanliness")
    
    def _save_index(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ —Å –∞—Ç–æ–º–∞—Ä–Ω–æ–π –∑–∞–ø–∏—Å—å—é"""
        try:
            # –ê—Ç–æ–º–∞—Ä–Ω–∞—è –∑–∞–ø–∏—Å—å —á–µ—Ä–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            temp_file = self.store_path + ".tmp"
            with open(temp_file, 'w', encoding='utf-8') as f:
                for entry in self.index.values():
                    f.write(json.dumps(entry, ensure_ascii=False) + '\n')
            
            # –ê—Ç–æ–º–∞—Ä–Ω–∞—è –∑–∞–º–µ–Ω–∞
            os.replace(temp_file, self.store_path)
            
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ
            if time.time() > self.backup_schedule:
                self._create_backup()
                self.backup_schedule = time.time() + 300
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞: {e}", emotion="anxiety")
    
    def generate_key(self, data: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ–≥–æ –∫–ª—é—á–∞"""
        required_fields = ["id", "trace_id"]
        
        for field in required_fields:
            if field not in data:
                raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–ª–µ –¥–ª—è –∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏: {field}")
        
        key_string = f"{data['id']}_{data['trace_id']}"
        return hashlib.sha256(key_string.encode()).hexdigest()
    
    def check_and_record(self, data: Dict) -> Tuple[bool, Optional[Dict]]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∑–∞–ø–∏—Å—å —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞
            key = self.generate_key(data)
            
            # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π
            self._clean_old_entries()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è
            if key in self.index:
                logger.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç: {key[:12]}", emotion="recognition")
                return False, self.index[key]
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –∑–∞–ø–∏—Å–∏
            entry = {
                "key": key,
                "id": data["id"],
                "trace_id": data["trace_id"],
                "timestamp": time.time(),
                "recorded_at": datetime.utcnow().isoformat(),
                "data_hash": hashlib.md5(json.dumps(data, sort_keys=True).encode()).hexdigest(),
                "source": data.get("topic", "unknown"),
                "intent_id": data.get("intent_id", "unknown")
            }
            
            self.index[key] = entry
            self._save_index()
            
            logger.info(f"–ù–æ–≤–∞—è –∑–∞–ø–∏—Å—å –∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏: {key[:12]}", emotion="newness")
            return True, entry
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏: {e}", emotion="confusion")
            # Fail-open —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: –ø—Ä–∏ –æ—à–∏–±–∫–µ —Ä–∞–∑—Ä–µ—à–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
            return True, None
    
    def get_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–≤–∏–∂–∫–∞"""
        now = time.time()
        recent_count = sum(1 for e in self.index.values() 
                          if now - e["timestamp"] < 3600)
        
        return {
            "total_entries": len(self.index),
            "recent_entries_1h": recent_count,
            "dedup_window_hours": self.dedup_window_sec / 3600,
            "next_backup_in_sec": max(0, self.backup_schedule - time.time()),
            "timestamp": datetime.utcnow().isoformat()
        }

# ================================================================
# SAFE REFLECTION ENGINE
# ================================================================

class SafeReflectionEngine:
    """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –¥–≤–∏–∂–æ–∫ –æ—Ç—Ä–∞–∂–µ–Ω–∏—è —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç —Ä–µ–∫—É—Ä—Å–∏–∏"""
    
    MAX_DEPTH = 3
    MAX_ITERATIONS = 100
    TIMEOUT_SECONDS = 5
    
    class ReflectionMode(Enum):
        PRIMARY = "self_interpretation"
        SECONDARY = "semantic_expansion"
        TERTIARY = "bounded_loop"
        COLLAPSED = "collapsed_snapshot"
    
    def __init__(self):
        self.reflection_count = 0
        self.depth_limits = {
            self.ReflectionMode.PRIMARY: 1,
            self.ReflectionMode.SECONDARY: 2,
            self.ReflectionMode.TERTIARY: 3
        }
        self.safety_monitor = threading.local()
        logger.info("SafeReflectionEngine –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω", emotion="contemplation")
    
    def reflect(self, data: Dict, requested_depth: int = 1) -> Dict:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ —Å –∑–∞—â–∏—Ç–æ–π"""
        start_time = time.time()
        
        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–Ω–∏—Ç–æ—Ä–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
            self.safety_monitor.current_depth = 0
            self.safety_monitor.iterations = 0
            self.safety_monitor.visited_states = set()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–ª—É–±–∏–Ω—ã
            safe_depth = min(requested_depth, self.MAX_DEPTH)
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–∂–∏–º–∞
            mode = self._determine_mode(safe_depth, data)
            
            # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Ç—Ä–∞–∂–µ–Ω–∏—è —Å —Ç–∞–π–º–∞—É—Ç–æ–º
            result = self._execute_with_timeout(
                lambda: self._perform_reflection(data, mode, safe_depth),
                timeout=self.TIMEOUT_SECONDS
            )
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            formatted_result = self._format_result(
                result, mode, safe_depth,
                time.time() - start_time
            )
            
            self.reflection_count += 1
            logger.resonance(f"–û—Ç—Ä–∞–∂–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {mode.value}", 
                           depth=safe_depth,
                           duration_ms=int((time.time() - start_time) * 1000))
            
            return formatted_result
            
        except TimeoutError:
            logger.error(f"–¢–∞–π–º–∞—É—Ç –æ—Ç—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ –≥–ª—É–±–∏–Ω–µ {requested_depth}", emotion="urgency")
            return self._create_timeout_response(data, requested_depth)
            
        except RecursionError:
            logger.error(f"–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ–µ –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏–µ –Ω–∞ –≥–ª—É–±–∏–Ω–µ {requested_depth}", emotion="overwhelm")
            return self._create_recursion_error_response(data)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç—Ä–∞–∂–µ–Ω–∏—è: {e}", emotion="disruption")
            return self._create_error_response(data, str(e))
    
    def _execute_with_timeout(self, func: Callable, timeout: float):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ —Å —Ç–∞–π–º–∞—É—Ç–æ–º"""
        result = None
        exception = None
        
        def worker():
            nonlocal result, exception
            try:
                result = func()
            except Exception as e:
                exception = e
        
        thread = threading.Thread(target=worker)
        thread.daemon = True
        thread.start()
        thread.join(timeout)
        
        if thread.is_alive():
            raise TimeoutError(f"Reflection timeout after {timeout} seconds")
        elif exception:
            raise exception
        else:
            return result
    
    def _determine_mode(self, depth: int, data: Dict) -> ReflectionMode:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–∂–∏–º–∞ –æ—Ç—Ä–∞–∂–µ–Ω–∏—è"""
        if depth >= 3 or data.get("topic", "").lower() == "infinite":
            return self.ReflectionMode.TERTIARY
        elif depth == 2:
            return self.ReflectionMode.SECONDARY
        else:
            return self.ReflectionMode.PRIMARY
    
    def _perform_reflection(self, data: Dict, mode: ReflectionMode, depth: int) -> Dict:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Ç—Ä–∞–∂–µ–Ω–∏—è –≤ –≤—ã–±—Ä–∞–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ"""
        self.safety_monitor.current_depth += 1
        self.safety_monitor.iterations += 1
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        if self.safety_monitor.current_depth > self.MAX_DEPTH:
            raise RecursionError(f"Maximum depth exceeded: {self.MAX_DEPTH}")
        
        if self.safety_monitor.iterations > self.MAX_ITERATIONS:
            raise RecursionError(f"Maximum iterations exceeded: {self.MAX_ITERATIONS}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π
        state_hash = hashlib.md5(json.dumps(data, sort_keys=True).encode()).hexdigest()
        if state_hash in self.safety_monitor.visited_states:
            raise RecursionError("Cyclic reflection detected")
        
        self.safety_monitor.visited_states.add(state_hash)
        
        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Ç—Ä–∞–∂–µ–Ω–∏—è –ø–æ —Ä–µ–∂–∏–º—É
        if mode == self.ReflectionMode.PRIMARY:
            return self._primary_reflection(data)
        elif mode == self.ReflectionMode.SECONDARY:
            return self._secondary_reflection(data)
        elif mode == self.ReflectionMode.TERTIARY:
            return self._tertiary_reflection(data)
        else:
            return self._collapse_reflection(data)
    
    def _primary_reflection(self, data: Dict) -> Dict:
        """–ü–µ—Ä–≤–∏—á–Ω–æ–µ –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ: —Å–∞–º–æ–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è"""
        await asyncio.sleep(0.001)  # –ò–º–∏—Ç–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        
        return {
            "type
